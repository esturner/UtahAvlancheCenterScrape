{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3c504321-0660-42fc-ac25-70756195a413",
   "metadata": {},
   "source": [
    "## Collecting Avalanche Forecasts\n",
    "\n",
    "Avalanche forecasts are quantized risk factors that are determined by the avalanche forecast professionals of the UAC. The risk factors range between 1-5 in severity: Low (1, Green), Moderate (2, Yellow), Considerable (3, Orange), High (4, Red), and Extreme! (5, Black). These are calculated by weighing various avalanche problems such as large storms which leads to storm slabs, persistant weak layers from weather variations, and sudden warming conditions which leads to wet slides. Each avalanche problem posted includes the information:\n",
    "\n",
    "- Type (Rising Temps, New Snow, Depth Hoar)\n",
    "- Location (Aspect and elevation in form of the compass rose. )\n",
    "- Likelihood (5-point scale from Unlikely (1) to Likely (3) to Certain (5))\n",
    "- Size (5-point scale from Small (1) to Medium (3) to Large (5))\n",
    "- Description\n",
    "\n",
    "This is the primary format of avalanche centers throughout the states and would be the most familiar one to present the findings and assesments we hope to generate from observations in a given day. To pick apart this we need to first figure out how to web scrape the Forecast page and the Archived forecast page which has forecasts accross the state starting from 2001 (https://utahavalanchecenter.org/archives/forecasts). The current forecasts are also displayed here.  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ed2f963-dcff-4026-94f2-e789840883eb",
   "metadata": {},
   "source": [
    "### Forecast List\n",
    "\n",
    "Using the archive link (https://utahavalanchecenter.org/archives/forecasts) we can get forecasts from the present day to 2018. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "623eed33-5eee-4c0b-8db1-7f7bff97ea7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "avy_center_url = 'http://utahavalanchecenter.org'\n",
    "forecast_archive_url = avy_center_url + '/archives/forecasts'\n",
    "page = requests.get(forecast_archive_url)\n",
    "soup = BeautifulSoup(page.content, 'html.parser')\n",
    "tbl = soup.find(\"table\")\n",
    "#print(tbl.dtype)\n",
    "page_forecasts = pd.read_html(str(tbl),extract_links ='all')[0]\n",
    "page_forecasts.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74d23b1d-1679-41e4-b217-2fde6da10675",
   "metadata": {},
   "source": [
    "This is practically the same as collecting observation data except we just substitute the different urls. We had get_table_obs() before so lets make a get_forecasts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5e03863-9512-4c45-86f4-0551cd3277eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_page_table(url):\n",
    "    '''returns a dataframe of avalanche observations from url. Data in df\n",
    "    includes Date, Region, Avalanche/Observation, (url) extension, and\n",
    "    observor'''\n",
    "    page = requests.get(url)\n",
    "    soup = BeautifulSoup(page.content, 'html.parser')\n",
    "    tbl=soup.find(\"table\")\n",
    "    page_obs = pd.read_html(str(tbl), extract_links='all')[0]\n",
    "    page_obs = clean_page_obs(page_obs)\n",
    "    return page_obs\n",
    "\n",
    "def clean_page_obs(page_obs):\n",
    "    '''Cleans up the html parser's interpretation of the observation list'''\n",
    "    old_columns = page_obs.columns\n",
    "    #change names \n",
    "    page_obs[['Date', 'a']]= pd.DataFrame(page_obs[old_columns[0]].tolist(), index=page_obs.index)\n",
    "    page_obs[['Region', 'b']]= pd.DataFrame(page_obs[old_columns[1]].tolist(), index=page_obs.index)\n",
    "    page_obs[['Observation Title', 'extension']]= pd.DataFrame(page_obs[old_columns[2]].tolist(), index=page_obs.index)\n",
    "    page_obs[['Observer', 'd']]= pd.DataFrame(page_obs[old_columns[3]].tolist(), index=page_obs.index)\n",
    "    #remove old columns & columns with none\n",
    "    page_obs=page_obs.drop(old_columns, axis=1)\n",
    "    page_obs=page_obs.drop(['a','b','d'], axis=1)\n",
    "    return page_obs\n",
    "\n",
    "page_obs = get_page_obs(observations_url)\n",
    "page_obs.head()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
